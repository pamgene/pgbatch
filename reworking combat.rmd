---
title: "Appendix A, description of the ComBat procedure"
author: "Rik de Wijn"
date: "9/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(data.table)
```

## Introduction
- This is a technical description of the Combat method for batch correction of micro array data by Johnson et al. (Biostatistics (2007) 8 p118-127). The goal is to calculate a batch correction for *N* x *G* data matrix **X** with *N* observations and *G* spots, where the observations are subdivided in multiple batches between which a batch effect occurs (or may occur). **bx** is the batch indicator vector of length *N*.

- Below the original ComBat method estimating an L/S model without designated reference batch is described

- Two optional modifications of the ComBat model have been introduced by Zhang et al. (BMC bioinformatics (2018) 19:226) that are of practical importance:
-- Estimating a Location-model (mean.only) rather than a Location and Scale (L/S) model.
-- Use of a designated reference batch that will not be changed by the correction; the other batches will be corrected *towards* the reference batch.

```{r}
bx = factor(c(1,1,1,1,2,2,2,2,2,3,3,3)) #12 observations divided over 3 batches
Y = matrix(nrow = length(bx), ncol = 100, data = rnorm(100*length(bx))) # 100 spots
```


The goal is to find a location and scale (L/S) adjustment for systematic effects between the batches.

Let $Y_{ijg}$ represent the signal for spot *g* from observation *j* in batch *i*, the L/S model is

$Y_{ijg} = \alpha_g + \lambda_{ig} + \sigma_{ig}*\epsilon_{ijg}$

$\alpha_g$, overall signal for spot *g*

$\lambda_{ig}$, additive batch effect (location) of batch *i*

$\sigma_{ig}$, multiplicative batch effect (scale) of batch *i*

$\epsilon_{ijg}$, residual error (assumed normal) for spot *g* in observation *j* in batch *i*

Note that here the term for covariates used by Johnson et al. has been dropped.

The batch adjusted data $Y^*$ is then given by:

$Y^*_{ijg} = \frac{Y_{ijg} - \alpha_g - \lambda_{ig}}{\sigma_{ig}} + \alpha_g$

, and we have to find the estimators $\hat{\alpha_g}$, $\hat{\lambda_{ig}}$, and $\hat{\sigma_{ig}}$

## Empirical Bayes Method for finding the batch adjustment

### Step 1. standardize the data

$Z_{ijg} = \frac{Y_{ijg} - \hat{\alpha_g}}{\hat{\sigma_g}}$

Where the *overall* L estimator $\hat{\alpha_g}$ is obtained using an ordinary least squares approach, i.e. numerically solve (for ${\lambda_{ig}}$) the system of equations:

$( B.B')\hat{\lambda_{ig}} = B'Y$, where $B$ is the batch design matrix.

$\hat{\lambda_{ig}}$ represents an estimate of the  mean per spot per batch. For each spot $\hat{\alpha_g}$ is then calculated as a weighted mean over the batches.

```{r alpha_g}
nObsPerBatch = summary(bx)
nObs = sum(nObsPerBatch)

B = model.matrix(~-1 + bx)
lamb.hat = solve( t(B)%*% B, t(B) %*% Y)
alpha_g = (nObsPerBatch/nObs) %*% lamb.hat
```

*Overall* variance $\hat{\sigma_g}$ is estimated as a pooled variance over the batches,

$\hat{\sigma_g}^2 = \frac{1}{N}\sum_{ij}(Y_{ijg}-\hat{\alpha_g}-\hat{\lambda_{ig}})^2$

and $Z_{ijg}$ calculated.

```{r Z}
siggsq  = t((Y - (B %*% lamb.hat))^2) %*% rep(1/nObs, nObs)
Z = scale(Y, center = alpha_g, scale = sqrt(siggsq))
View(Z)
```


### Step 2. Empirical Bayes estimate of batch effect parameters using parametric emprirical priors
Scaled data Z is used as input for estimating the EB batch correction. The EB corrected data is given by:
$Y_{ijg}^* = \hat{\sigma_g}\frac{(Z_{ijg} - \hat{\lambda_{ij}^*})}{\hat{\sigma_{ig}^*}} + \hat{\alpha_g}$

$\hat{\lambda_{ig}^*}$ is the adjusted estimator for the batch location effect and $\hat{\sigma_{ig}^*}$ the adjusted estimator for the batch scale effect (Adjusted compared the per spot estimators $\hat{\lambda_{ig}}$ and $\hat{\sigma_{ig}}$ used above). The adjusted estimators are given by:

$\hat{\lambda_{ig}^*} = \frac{n_i\bar{\tau_i^2}\hat{\lambda_{ig}} + \sigma_{ig}^{2*}\bar{\lambda_i}}{n_i\bar{\tau_i^2} + \sigma_{ig}^{2*}}$ and $\sigma_{ig}^{2*}= \frac{\bar{\theta_i}+\frac{1}{2}\sum_j(Z_{ijg} - \lambda_{ig}^*)^2}{\frac{n_j}{2} + \bar{\gamma_i} - 1}$

Where $\lambda_i, \tau_i^2, \gamma_i, \theta_i$ are the parameters of the assumed prior distributions for the location (normal) and scale (inverse gamma) effect,

$\lambda_{ig} \sim N(\lambda_i, \tau_i^2)$ and $\sigma_{ig}^2 \sim \Gamma^{-1}(\gamma_i, \theta_i)$

and are estimated from the scaled data Z. With these parameters obtained the equations for $\hat{\lambda_{ig}^*}$ and $\sigma_{ig}^{2*}$ can be solved by a simple iterative procedure.

```{r EB}


aprior = function(X) {
      m <- mean(X)
      s2 <- var(X)
      (2*s2 + m^2) / s2
}

bprior =  function(X){
      m <- mean(X)
      s2 <- var(X)
      (m*s2 + m^3) / s2
}

postmean =  function(g.hat,g.bar,n,d.star,t2){
  (t2*n*g.hat + d.star*g.bar) / (t2*n + d.star)
}

postvar = function(sum2,n,a,b){
  (.5*sum2 + b) / (n/2 + a - 1)
}
it.sol = function(params, Z, lambda.hat, sigma.hat, conv = .0001)
{
   g.old = lambda.hat
   d.old = sigma.hat
   change <- 1
   count <- 0
   while(change>conv){
      g.new <- postmean(lambda.hat, params$lambda.bar, params$n, d.old, params$t2)
      sum2 = colSums(scale(Z, center = g.new, scale = FALSE)^2)
      d.new <- postvar(sum2, params$n, params$gamma, params$theta)
      change <- max(abs(g.new-g.old) / g.old, abs(d.new-d.old) / d.old)
      g.old <- g.new
      d.old <- d.new
      count <- count+1
   }
   cat("This batch took", count, "iterations until convergence\n")
   result = data.table(lambda.star = list(g.new), sigma.star = list(d.new))
}


lambda.hat = solve( t(B)%*% B, t(B) %*% Z) # unadjusted location
sigma.hat = NULL
for(i in 1:nlevels(bx)){
     sigma.hat = rbind(sigma.hat, apply(Z[bx == levels(bx)[i],,drop = FALSE],2,var))
} # unadjusted scale

params = data.frame( bx = levels(bx),
                     bi = 1:nlevels(bx),
                     lambda.bar = rowMeans(lambda.hat),
                     t2 = apply(lambda.hat,1, var),
                     gamma = apply(sigma.hat, 1, aprior),
                     theta = apply(sigma.hat, 1, bprior),
                     n = summary(bx))

# solving for batch effect
post = params %>% group_by(bx) %>% do(it.sol(., Z = Z[bx == .$bx,], lambda.hat[.$bi,], sigma.hat[.$bi,]))
```
### Step 3. Return batch corrected data
```{r correct}

batchcorrect = function(Z, bx,  post, lambda_g, sigmasq_g){
  Ystar = matrix(nrow = dim(Z)[1], ncol = dim(Z)[2])
  for (i in 1:dim(Z)[1]){
    bIdx = (1:nlevels(bx))[bx[i] == levels(bx)]
    zstar= (Z[i,] - post$lambda.star[[bIdx]])/sqrt(post$sigma.star[[bIdx]])
    Ystar[i,] = sqrt(sigmasq_g) * zstar + lambda_g
  }
  return(Ystar)
}

#Ystar = batchcorrect(Z, bx, post, alpha_g, siggsq)

```

In the pgBatch package (https://rikdewijn@bitbucket.org/bnoperator/pgbatch.git) the procedure is implemented using a class with a "Fit" and "Apply" method. Fit method returns an object with teh correction parameters, Apply method can be used to apply the correction to a data set. In this way a ComBat model fitted on data set $Y_1$ (i.e. REF samples), can be applied to set $Y_2$ (DAS samples). A clean version of the code is provided in "combat_reworked.r".

## Reference batch
- Data scaling is performed based on the data in the reference set
